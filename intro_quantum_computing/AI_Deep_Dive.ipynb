{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba40f7be",
   "metadata": {},
   "source": [
    "### **Train an RL Agent Using Q-Learning**\n",
    "Problems arose\n",
    "* I didn't stick around long enough on the code\n",
    "* gym updated content and syntax\n",
    "* takes over 3 min to run script\n",
    "* didn't want to waste time waiting moved on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e95831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create Environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "# Get size of state and action space\n",
    "state_size = env.observation_space.n  # Number of states (16 for 4x4 FrozenLake)\n",
    "action_size = env.action_space.n      # Number of actions (4: left, down, right, up)\n",
    "\n",
    "# Reset environment (unpack observation and info)\n",
    "state, info = env.reset()\n",
    "\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Action size: {action_size}\")\n",
    "print(f\"Starting state: {state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((state_size,action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8230c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Learning rate\n",
    "gamma = 0.99 # Discount factor\n",
    "epsilon = 1.0 # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "episodes = 2000 # Number of games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b312f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3772713",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choose action using epsilon-greedy strategy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(action_size) # Exploration\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state]) # Exploitation\n",
    "\n",
    "        # Take action and observe reward\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Update Q-table using Bellman equation\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q_table[next_state]) - Q_table[state,action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9fb036",
   "metadata": {},
   "source": [
    "# **Since Gym updated software and syntax**\n",
    "i had to optimize the above code with ai to understand how to use the new syntax so here is the optimized code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5add81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# Create Environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "# Get size of state and action space\n",
    "state_size = env.observation_space.n  # 16 states for 4x4 FrozenLake\n",
    "action_size = env.action_space.n      # 4 possible actions\n",
    "\n",
    "# Initialize Q-table\n",
    "Q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1          # Learning rate\n",
    "gamma = 0.99         # Discount factor\n",
    "epsilon = 1.0        # Exploration rate (start)\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "episodes = 2000      # Total episodes for training\n",
    "\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Action size: {action_size}\")\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, info = env.reset()  # Gymnasium style reset returns tuple\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(action_size)  # exploration\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])      # exploitation\n",
    "\n",
    "        # Take action, observe reward and next state\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update Q-table using Q-learning update rule (Bellman equation)\n",
    "        Q_table[state, action] += alpha * (\n",
    "            reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# Clean up environment resources\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b174c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# Create Environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "# Get size of state and action space\n",
    "state_size = env.observation_space.n  # Number of states (16 for 4x4 FrozenLake)\n",
    "action_size = env.action_space.n      # Number of actions (4: left, down, right, up)\n",
    "\n",
    "# Initialize Q-table\n",
    "Q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1          # Learning rate\n",
    "gamma = 0.99         # Discount factor\n",
    "epsilon = 1.0        # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "episodes = 2000      # Number of episodes to train\n",
    "\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Action size: {action_size}\")\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, info = env.reset()   # Gymnasium style reset\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choose action using epsilon-greedy strategy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(action_size)  # Exploration\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])      # Exploitation\n",
    "\n",
    "        # Take action and observe reward and next state\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated              # Gymnasium done signal\n",
    "\n",
    "        # Update Q-table using Bellman equation\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay epsilon (exploration rate)\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# Close environment to clean up resources\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
